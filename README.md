# Systolic Array Matrix Multiplication IPs for SLM Inference Engine on Edge

## ğŸ“˜ Introduction

Current **Small Language Model (SLM)** inference solutions for edge devices face major performance bottlenecks. When deployed on **RV32-based edge processors**, these limitations become even more severe due to:

- Limited compute resources, memory bandwidth, and lack of specialized hardware
- Software-only inference pipelines that are inefficient under edge constraints

The challenge is to develop a **reliable, lightweight, and flexible inference methodology** that supports full SLM inference on RV32 platforms.

### ğŸ¯ Goal
Create a **lightweight FPGA-based accelerator IP** and **inference Engine** that offloads GEMM operations from the CPU, providing **significant speedup** on resource-constrained RISC-V edge devices.

### ğŸ† Contributions

1. **Lightweight, scalable accelerator (GEMMA IP)**
   - Accelerates INT8 GEMM operations  
   - Supports tiling to reduce external memory traffic  
   - Scalable systolic designs (INT8, INT4, FP16 variants)

2. **Bare-metal inference engine framework**
   - Fully sequential runtime  
   - Flexible and extensible  
   - Cross-platform verification  
   - Designed for RV32 SoCs and FPGA platforms

---

## ğŸ› ï¸ Proposed Methodology

### 1. Systolic Array Architecture (16Ã—16)

Each Processing Element (PE):

- Performs signed **INT8 Ã— INT8** multiplication  
- Accumulates into **32-bit sum**  
- Receives weights from **north** and activations from **west**  
- Introduces a one-cycle delay between adjacent PEs  

This enables highly parallelized, pipelined matrix multiplication suitable for FPGA.

---

### 2. Tiled Matrix Multiplication

Large matrices (e.g., **1024Ã—1024**) are decomposed into **16Ã—16 tiles**, enabling:

- Burst DMA transfers of tiles  
- On-chip buffering  
- **Double buffering** for overlapped:
  - Tile fetch  
  - Computation  
  - Result writeback  

Tile partial sums are accumulated until forming the full result matrix.

---

### 3. AXI Integration + Inference Engine

- **AXI4-Lite** for CPU-controlled registers  
- **AXI4 Master** for DMA-based reads/writes  
- Bare-metal inference runtime handles:
  - Custom memory allocation  
  - Tiling schedule  
  - Accelerator synchronization  

### âœ… Advantages

- Latency reduced **from minutes to seconds**
- Open-source RISC-V + FPGA platform
- Energy-efficient and highly scalable
- Straightforward extension to larger systolic arrays or mixed-precision inference

---

# ğŸ“ Architecture Diagrams

### Systolic Array Block Diagram
![systolic array architecture diagram](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/sys_block.png)

### Tiling Architecture (32Ã—32 using 16Ã—16 tiles)
![Tiling architecture diagram](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/Acc_IP.png)

### Accelerator Integration with RISC-V + AXI
![acceleration block](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/systolic_soc_implementation.png)

---
## ğŸš€ Features

- **High-throughput INT8 systolic array** accelerators (16Ã—16, 32Ã—32)
- **Tiling support**: large matrix multiplication using smaller arrays (e.g., 32Ã—32 via 16Ã—16 tiles)
- **Scalable systolic arrays**: INT4/FP16 variants, parametrized dimensions
- **AXI-Lite + AXI4 interfaces** for CPU-controlled SoC integration
- **Full Vivado testbenches** for simulation and verification
- **Integration with VEGA SoC** for real benchmarks
- Includes **waveform captures, benchmark results, and block diagrams** for documentation

## ğŸ“‚ Repository Structure

```
Systolic_Array_Matmul_for_Gemma3_Acc/
â”œâ”€â”€ Accelerator_IP/
â”‚   â”œâ”€â”€ Gemma_Accelerator_IP/
â”‚   â”‚   â”œâ”€â”€ INT8_16x16/                    # 16x16 INT8 systolic array IP
â”‚   â”‚   â”œâ”€â”€ INT8_32x32/                    # 32x32 INT8 systolic array IP
â”‚   â”‚   â””â”€â”€ Tiling/                        # Tiling implementation (32x32 using 16x16)
â”‚   â””â”€â”€ Systolic_array_IP/
â”‚       â”œâ”€â”€ Scalable_sytolic_matmul_axi/
â”‚       â”‚   â”œâ”€â”€ INT4_INT4/
â”‚       â”‚   â”‚   â”œâ”€â”€ Final_DMA_v2/          # DMA-enabled version
â”‚       â”‚   â”‚   â””â”€â”€ Final_v1/              # Basic version
â”‚       â”‚   â””â”€â”€ f16_INT4/                  # FP16Ã—INT4 implementation
â”‚       â”œâ”€â”€ not_scalable_systolic_matmul/  # Initial fixed-size design
â”‚       â””â”€â”€ scalable_systolic_matmul/      # Parameterized systolic arrays
â”œâ”€â”€ Application/
â”‚   â””â”€â”€ INT8_16x16/
â”‚       â”œâ”€â”€ benchmark.c                    # Performance benchmarking code
â”‚       â”œâ”€â”€ host.c                         # Host-side control software
â”‚       â”œâ”€â”€ main.c                         # Main application entry point
â”‚       â””â”€â”€ matmul_offload.c              # Matrix multiplication offload functions
â”œâ”€â”€ .gitattributes
â””â”€â”€ README.md
```

## ğŸ–¼ï¸ Architecture Overview

### Systolic Array Block Diagram
![systolic array architecture diagram](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/sys_block.png)


The systolic array consists of a grid of Processing Elements (PEs) that perform matrix multiplication in a pipelined fashion. Data flows through the array in a wave-like pattern, maximizing computational throughput.

### Tiling Architecture (32Ã—32 via 16Ã—16 tiles)
![Tilling architecture diagram](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/Acc_IP.png)

The tiling implementation allows larger matrix multiplications by decomposing them into smaller sub-matrices that fit within the 16Ã—16 systolic array, then combining the results.

## ğŸ§© Key RTL Modules

### Gemma Accelerator IP
- **`gemma_accelerator.v`** - Top-level module with AXI-Lite control registers and AXI4 memory interfaces
- **`systolic_array_16x16.v`** - Configurable 16Ã—16 systolic array grid
- **`pe_int8.v`** - Processing element: INT8Ã—INT8 multiply with 32-bit accumulation
- **`accelerator_buffer.v`** - Input/output buffers for A, B matrices and result staging

### Systolic Array IP Variants
- **Scalable Implementation**: Parameterized systolic arrays supporting various dimensions
- **INT4 Support**: Optimized for quantized neural network inference
- **FP16Ã—INT4**: Mixed-precision multiplication for enhanced accuracy
- **DMA Integration**: Direct memory access for improved data transfer efficiency

## ğŸ“Š Benchmarks & Results

### Simulation and Implementation Results

#### INT8 16Ã—16 IP on VEGA Processor SoC
- âœ… **Simulation verified** (waveform screenshots below)
![IP Wave - Data flow](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/IP1.png)
![IP Wave - Results](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/IP2.png)

- âœ… **Hardware implementation successful - Benchmark**
![Systolic SOC Benchmark](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/systolic_soc_implementation.png)

- âœ… **Hardware implementation successful - Offload**
![Systolic SOC offload](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/offload_result.jpeg)


#### Tiling IP (32Ã—32 using 16Ã—16 systolic array) on SoC
- âœ… **Implementation Results**
![Systolic SOC Benchmark](https://github.com/PrabathBK/Systolic_Array_Matmul_for_Gemma3_Acc/blob/main/Results/Tiling_log.png)



#### Integration with VEGA Processor

1. **Load data into memory** (A, B matrices)
2. **Configure accelerator via AXI-Lite**:
   - Write base addresses (A, B, C matrices)
   - Set matrix dimensions
   - Set start bit
3. **Wait for completion** (done flag or interrupt)
4. **Read results from memory** (C matrix)


## ğŸ”§ Configuration Options

### Compile-time Parameters
- `ARRAY_SIZE`: Systolic array dimensions (default: 16)
- `DATA_WIDTH`: Input data width (8 for INT8, 4 for INT4)
- `ACCUMULATOR_WIDTH`: Output accumulator width (default: 32)
- `BUFFER_DEPTH`: Input/output buffer depth

### Runtime Configuration
- Matrix dimensions (up to maximum supported by array size)
- Memory addresses for input/output matrices
- Interrupt enable/disable
- Debug mode enable/disable

## ğŸš€ Performance Optimization Tips

1. **Memory Layout**: Use contiguous memory allocation for better cache performance
2. **Data Alignment**: Align matrices to cache line boundaries
3. **Tiling Strategy**: Choose tile sizes that maximize array utilization
4. **Pipeline Optimization**: Overlap data loading with computation when possible


**Note**: This accelerator is designed specifically for the Gemma LLM inference pipeline and may require modifications for other neural network architectures.
